{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating and loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THESE COMMANDS ONLY WORK IN CLOUDERA VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open this link for picture output\n",
    "\n",
    "https://pysparktutorials.wordpress.com/writing-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "launch the spark session in cloudera using the below command\n",
    "\n",
    "pyspark --packages com.databricks:spark-avro_2.10:2.0.1\n",
    "\n",
    "'''\n",
    "# loading the data and assigning the schema.\n",
    "\n",
    "path_text_orders=\"/user/pruthviraj/sqoop_text/orders\"\n",
    "\n",
    "orders_text=sqlContext.read.format(\"text\").load(path_text_orders)\n",
    "\n",
    "orders_table=orders_text.selectExpr(\"cast(split(value,',') [0] as int) order_customer_id\",\n",
    "                                     \"cast(split(value,',') [1] as date) order_date\",\n",
    "                                     \"cast(split(value,',') [2] as int) order_id\",\n",
    "                                      \"cast(split(value,',') [3] as string) order_status\")\n",
    "\n",
    "orders_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing without compression of text\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "orders_table.select(f.concat_ws(\",\",\"order_customer_id\",\"order_date\",\"order_id\",\"order_status\")).\\\n",
    "                    write.format(\"text\").save(\"/user/pruthviraj/text\")\n",
    "import os\n",
    "os.system(\"hdfs dfs -ls /user/pruthviraj/text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing with compression of text\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "orders_table.select(f.concat_ws(\",\",\"order_customer_id\",\"order_date\",\"order_id\",\"order_status\")).\\\n",
    "                   rdd.saveAsTextFile(\"/user/pruthviraj/text_compress\",\"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "import os\n",
    "os.system(\"hdfs dfs -ls /user/pruthviraj/text_compress\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write json without compression\n",
    "\n",
    "orders_table.write.format('json').save(\"/user/pruthviraj/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing json with compression\n",
    "\n",
    "orders_table.toJSON().saveAsTextFile(\"/user/pruthviraj/json_compress\",\\\n",
    "                                     \"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parquet without compression\n",
    "\n",
    "orders_table.write.format('parquet').save(\"/user/pruthviraj/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parquet with compression\n",
    "\n",
    "# sqlContext.setConf(\"spark.sql.parquet.compression.codec\",\"{gzip,snappy,uncompressed}\")\n",
    "\n",
    "sqlContext.setConf(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "\n",
    "orders_table.write.format('parquet').save(\"/user/pruthviraj/parquet_compress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you may not see the compression extension but if you compare the size you will realize that compression has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## writing avro without compression\n",
    "\n",
    "# avro format did not recognize the date format so we have reloaded the data\n",
    "\n",
    "path_text_orders=\"/user/pruthviraj/sqoop_text/orders\"\n",
    "\n",
    "orders_text=sqlContext.read.format(\"text\").load(path_text_orders)\n",
    "\n",
    "orders_table=orders_text.selectExpr(\"cast(split(value,',') [0] as int) order_customer_id\",\n",
    "                                     \"cast(split(value,',') [1] as string) order_date\",\n",
    "                                     \"cast(split(value,',') [2] as int) order_id\",\n",
    "                                      \"cast(split(value,',') [3] as string) order_status\")\n",
    "\n",
    "orders_table.write.format(\"com.databricks.spark.avro\").save(\"/user/pruthviraj/avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing avro with compression\n",
    "\n",
    "'''there is problem with cloudera vms. it wont execute snapyy compression and \n",
    "you will an error saying its not installed'''\n",
    "\n",
    "#sqlContext.setConf(\"spark.sql.avro.compression.codec\",{deflate,uncompressed,snappy})\n",
    "\n",
    "sqlContext.setConf(\"spark.sql.avro.compression.codec\",\"deflate\")\n",
    "\n",
    "path_text_orders=\"/user/pruthviraj/sqoop_text/orders\"\n",
    "\n",
    "orders_text=sqlContext.read.format(\"text\").load(path_text_orders)\n",
    "\n",
    "orders_table=orders_text.selectExpr(\"cast(split(value,',') [0] as int) order_customer_id\",\n",
    "                                     \"cast(split(value,',') [1] as string) order_date\",\n",
    "                                     \"cast(split(value,',') [2] as int) order_id\",\n",
    "                                      \"cast(split(value,',') [3] as string) order_status\")\n",
    "\n",
    "orders_table.write.format(\"com.databricks.spark.avro\").save(\"/user/pruthviraj/avro_compress\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
