{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use tis command if you are using the jupyter notebook\n",
    "\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").config(conf=SparkConf()).getOrCreate()\n",
    "\n",
    "# loading the data and assigning the schema.\n",
    "\n",
    "path_text_orders=\"file:///D://data-master/retail_db/orders\"\n",
    "\n",
    "orders_text=spark.read.format(\"text\").load(path_text_orders)\n",
    "\n",
    "orders_table=orders_text.selectExpr(\"cast(split(value,',') [0] as int) order_customer_id\",\n",
    "                                     \"cast(split(value,',') [1] as date) order_date\",\n",
    "                                     \"cast(split(value,',') [2] as int) order_id\",\n",
    "                                      \"cast(split(value,',') [3] as string) order_status\")\n",
    "\n",
    "orders_table.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching in CLOUDERA vm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to stop the warnings and info in saprk 1.6\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''launch pysaprk'''\n",
    "\n",
    "'''load the data'''\n",
    "\n",
    "path_text_orders=\"/user/pruthviraj/sqoop_text/orders\"\n",
    "\n",
    "orders_text=sqlContext.read.format(\"text\").load(path_text_orders)\n",
    "\n",
    "orders_table=orders_text.selectExpr(\"cast(split(value,',') [0] as int) order_customer_id\",\n",
    "                                     \"cast(split(value,',') [1] as date) order_date\",\n",
    "                                     \"cast(split(value,',') [2] as int) order_id\",\n",
    "                                      \"cast(split(value,',') [3] as string) order_status\")\n",
    "\n",
    "orders_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tilte](https://pysparktutorials.files.wordpress.com/2018/05/load-vm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  once you load the data , these commands should work on both jupyter notebook and cloudera vms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gruouping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy is case sensitive , please make a note of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|   order_status|count|\n",
      "+---------------+-----+\n",
      "|PENDING_PAYMENT|15030|\n",
      "|       COMPLETE|22899|\n",
      "|        ON_HOLD| 3798|\n",
      "| PAYMENT_REVIEW|  729|\n",
      "|     PROCESSING| 8275|\n",
      "|         CLOSED| 7556|\n",
      "|SUSPECTED_FRAUD| 1558|\n",
      "|        PENDING| 7610|\n",
      "|       CANCELED| 1428|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping the data and counting the elements\n",
    "\n",
    "orders_table.groupBy(\"order_status\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggretaing the data\n",
    "\n",
    "#### to aggreagate the data we need to import the pyspark sql functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+\n",
      "|   order_status|  cnt|  max|\n",
      "+---------------+-----+-----+\n",
      "|PENDING_PAYMENT|15030|12434|\n",
      "|       COMPLETE|22899|12434|\n",
      "|        ON_HOLD| 3798|12434|\n",
      "| PAYMENT_REVIEW|  729|12433|\n",
      "|     PROCESSING| 8275|12431|\n",
      "|         CLOSED| 7556|12434|\n",
      "|SUSPECTED_FRAUD| 1558|12429|\n",
      "|        PENDING| 7610|12435|\n",
      "|       CANCELED| 1428|12435|\n",
      "+---------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregating the data\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "orders_table.groupBy(\"order_status\").agg(f.count(orders_table.order_status).alias(\"count\"),\\\n",
    "                                         f.max(orders_table.order_id).alias(\"max\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# having cluase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there is no having clause in the pyspark ,the best you can make use of filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+\n",
      "|   order_status|count|  max|\n",
      "+---------------+-----+-----+\n",
      "|PENDING_PAYMENT|15030|12434|\n",
      "|       COMPLETE|22899|12434|\n",
      "|     PROCESSING| 8275|12431|\n",
      "|         CLOSED| 7556|12434|\n",
      "|        PENDING| 7610|12435|\n",
      "+---------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "orders_table.groupBy(\"order_status\").\\\n",
    "             agg(f.count(orders_table.order_status).alias(\"count\"),\\\n",
    "             f.max(orders_table.order_id).alias(\"max\")).\\\n",
    "             where(\"count > 5000\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
