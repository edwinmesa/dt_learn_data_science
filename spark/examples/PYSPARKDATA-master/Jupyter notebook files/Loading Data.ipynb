{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|1,2013-07-25 00:0...|\n",
      "|2,2013-07-25 00:0...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#available from spark 1.6 +\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "#load data to the orders_text\n",
    "\n",
    "path_text_orders=\"file:///D://data-master/retail_db/orders\"\n",
    "\n",
    "orders_text=spark.read.format(\"text\").load(path_text_orders)\n",
    "\n",
    "#First way is to use selectExper methos\n",
    "\n",
    "orders_text_scltexpr=orders_text.selectExpr(\"cast(split(value,',') [0] as int) order_customer_id\",\n",
    "                                            \"cast(split(value,',') [1] as date) order_date\",\n",
    "                                            \"cast(split(value,',') [2] as int) order_id\",\n",
    "                                            \"cast(split(value,',') [3] as string) order_status\")\n",
    "\n",
    "\n",
    "\n",
    "#second way is to use withColumn method . this will add extra cloumns to the dataframe , which will ensure to remove it\n",
    "orders_text_with_col=orders_text.withColumn(\"order_customer_id\",f.split(orders_text.value,\",\")[0].cast(\"int\")).\\\n",
    "                                 withColumn(\"order_date\",f.split(orders_text.value,\",\")[1].cast(\"date\")).\\\n",
    "                                 withColumn(\"order_id\",f.split(orders_text.value,\",\")[2].cast(\"int\")).\\\n",
    "                                 withColumn(\"order_status\",f.split(orders_text.value,\",\")[3].cast(\"string\")) \n",
    "\n",
    "orders_text_with_col=orders_text_with_col.select(\"order_customer_id\",\"order_date\",\"order_id\",\"order_status\")\n",
    "\n",
    "# the third way is to select the columns and aliasing them along with cast\n",
    "\n",
    "orders_text_split=orders_text.select(\n",
    "             f.split(orders_text.value,',')[0].cast('int').alias('order_customer_id'),\n",
    "             f.split(orders_text.value,',')[1].cast('date').alias('order_date'),\n",
    "             f.split(orders_text.value,',')[2].cast('int').alias('order_id'),\n",
    "             f.split(orders_text.value,',')[3].cast('string').alias('order_status')\n",
    "             )\n",
    "\n",
    "\n",
    "orders_text.show(2)\n",
    "orders_text_with_col.show(2)\n",
    "orders_text_scltexpr.show(2)\n",
    "orders_text_split.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format using CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------------+\n",
      "|_c0|                 _c1|  _c2|            _c3|\n",
      "+---+--------------------+-----+---------------+\n",
      "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
      "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
      "+---+--------------------+-----+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#available from spark 2 +\n",
    "\n",
    "#load data to the orders_csv \n",
    "\n",
    "path_csv_orders=\"file:///D://data-master/retail_db/orders\"\n",
    "\n",
    "orders_csv=spark.read.format(\"csv\").load(path_csv_orders)\n",
    "\n",
    "#First way is to use selectExper methos\n",
    "\n",
    "orders_csv_slctexpr=orders_csv.selectExpr(\"cast(_c0 as int) order_customer_id\",\n",
    "                                 \"cast(_c1 as date) order_date\",\n",
    "                                 \"cast(_c2 as int) order_id\",\n",
    "                                 \"cast(_c3 as string) order_status\")\n",
    "\n",
    "#second way is to use withColumn method . this will add extra cloumns to the dataframe , which will ensure to remove it\n",
    "\n",
    "orders_csv_with_col=orders_csv.withColumn(\"order_customer_id\",orders_csv._c0.cast(\"int\")).\\\n",
    "                               withColumn(\"order_date\",orders_csv._c1.cast(\"date\")).\\\n",
    "                               withColumn(\"order_id\",orders_csv._c2.cast(\"int\")).\\\n",
    "                               withColumn(\"order_status\",orders_csv._c3.cast(\"string\"))\n",
    "\n",
    "orders_csv_with_col=orders_csv_with_col.select(\"order_customer_id\",\"order_date\",\"order_id\",\"order_status\")\n",
    "\n",
    "# the third way is to select the columns and aliasing them along with cast\n",
    "\n",
    "orders_csv_cast=orders_csv.select(orders_csv._c0.cast(\"int\").alias(\"order_customer_id\"),\n",
    "                              orders_csv._c1.cast(\"date\").alias(\"order_date\"),\n",
    "                              orders_csv._c2.cast(\"int\").alias(\"order_id\"),\n",
    "                              orders_csv._c3.cast(\"string\").alias(\"order_status\"))\n",
    "orders_csv.show(2)\n",
    "orders_csv_slctexpr.show(2)\n",
    "orders_csv_with_col.show(2)\n",
    "orders_csv_cast.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using structures with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                1|2013-07-25|   11599|         CLOSED|\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "|                3|2013-07-25|   12111|       COMPLETE|\n",
      "|                4|2013-07-25|    8827|         CLOSED|\n",
      "|                5|2013-07-25|   11318|       COMPLETE|\n",
      "|                6|2013-07-25|    7130|       COMPLETE|\n",
      "|                7|2013-07-25|    4530|       COMPLETE|\n",
      "|                8|2013-07-25|    2911|     PROCESSING|\n",
      "|                9|2013-07-25|    5657|PENDING_PAYMENT|\n",
      "|               10|2013-07-25|    5648|PENDING_PAYMENT|\n",
      "|               11|2013-07-25|     918| PAYMENT_REVIEW|\n",
      "|               12|2013-07-25|    1837|         CLOSED|\n",
      "|               13|2013-07-25|    9149|PENDING_PAYMENT|\n",
      "|               14|2013-07-25|    9842|     PROCESSING|\n",
      "|               15|2013-07-25|    2568|       COMPLETE|\n",
      "|               16|2013-07-25|    7276|PENDING_PAYMENT|\n",
      "|               17|2013-07-25|    2667|       COMPLETE|\n",
      "|               18|2013-07-25|    1205|         CLOSED|\n",
      "|               19|2013-07-25|    9488|PENDING_PAYMENT|\n",
      "|               20|2013-07-25|    9198|     PROCESSING|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second method is to use the stucture to define the schema  \n",
    "\n",
    "path_csv_orders=\"file:///D://data-master/retail_db/orders\"\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"order_customer_id\", IntegerType(), True),\n",
    "                       StructField(\"order_date\", DateType(), True),\n",
    "                     StructField(\"order_id\", IntegerType(), True),\n",
    "                     StructField(\"order_status\", StringType(), True)])\n",
    "\n",
    "orders_csv=spark.read.csv(path_csv_orders,schema=schema, inferSchema=False)\n",
    "\n",
    "orders_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Parquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+---------------+\n",
      "|order_id|   order_date|order_customer_id|   order_status|\n",
      "+--------+-------------+-----------------+---------------+\n",
      "|       1|1374735600000|            11599|         CLOSED|\n",
      "|       2|1374735600000|              256|PENDING_PAYMENT|\n",
      "|       3|1374735600000|            12111|       COMPLETE|\n",
      "|       4|1374735600000|             8827|         CLOSED|\n",
      "|       5|1374735600000|            11318|       COMPLETE|\n",
      "|       6|1374735600000|             7130|       COMPLETE|\n",
      "|       7|1374735600000|             4530|       COMPLETE|\n",
      "|       8|1374735600000|             2911|     PROCESSING|\n",
      "|       9|1374735600000|             5657|PENDING_PAYMENT|\n",
      "|      10|1374735600000|             5648|PENDING_PAYMENT|\n",
      "|      11|1374735600000|              918| PAYMENT_REVIEW|\n",
      "|      12|1374735600000|             1837|         CLOSED|\n",
      "|      13|1374735600000|             9149|PENDING_PAYMENT|\n",
      "|      14|1374735600000|             9842|     PROCESSING|\n",
      "|      15|1374735600000|             2568|       COMPLETE|\n",
      "|      16|1374735600000|             7276|PENDING_PAYMENT|\n",
      "|      17|1374735600000|             2667|       COMPLETE|\n",
      "|      18|1374735600000|             1205|         CLOSED|\n",
      "|      19|1374735600000|             9488|PENDING_PAYMENT|\n",
      "|      20|1374735600000|             9198|     PROCESSING|\n",
      "+--------+-------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading parquet data\n",
    "orders=spark.read.format(\"parquet\").load(\"file:///d://pyspark/retail_db_parquet/orders\")\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|                2|2013-07-25|     256|PENDING_PAYMENT|\n",
      "|                4|2013-07-25|    8827|         CLOSED|\n",
      "|                5|2013-07-25|   11318|       COMPLETE|\n",
      "|               10|2013-07-25|    5648|PENDING_PAYMENT|\n",
      "|               12|2013-07-25|    1837|         CLOSED|\n",
      "|               13|2013-07-25|    9149|PENDING_PAYMENT|\n",
      "|               14|2013-07-25|    9842|     PROCESSING|\n",
      "|               18|2013-07-25|    1205|         CLOSED|\n",
      "|               22|2013-07-25|     333|       COMPLETE|\n",
      "|               25|2013-07-25|    9503|         CLOSED|\n",
      "|               28|2013-07-25|     656|       COMPLETE|\n",
      "|               31|2013-07-25|    6983| PAYMENT_REVIEW|\n",
      "|               32|2013-07-25|    3960|       COMPLETE|\n",
      "|               36|2013-07-25|    5649|        PENDING|\n",
      "|               37|2013-07-25|    5863|         CLOSED|\n",
      "|               38|2013-07-25|   11586|     PROCESSING|\n",
      "|               45|2013-07-25|    2636|       COMPLETE|\n",
      "|               46|2013-07-25|    1549|        ON_HOLD|\n",
      "|               49|2013-07-25|    1871|        PENDING|\n",
      "|               50|2013-07-25|    5225|       CANCELED|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading json data\n",
    "orders=spark.read.format(\"json\").load(\"file:///d://pyspark/retail_db_json_orders\")\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading csv data\n",
    "movies=spark.read.csv(\"file:///d://ml-20m/movies.csv\",header=True)\n",
    "movies.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
